{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Variable Linear Regression (Regresión lineal de múltiples variables)\n",
    "\n",
    "En este laboratorio, usted ampliará las estructuras de datos y las rutinas previamente desarrolladas para soportar múltiples características. Varias rutinas se actualizan haciendo que el laboratorio parezca largo, pero hace pequeños ajustes a las rutinas anteriores haciendo que sea rápido de revisar.\n",
    "\n",
    "# Esquema\n",
    "- [&nbsp;&nbsp;1.1 Objetivos](#toc_15456_1.1)\n",
    "- [&nbsp;&nbsp;1.2 Herramientas](#toc_15456_1.2)\n",
    "- [&nbsp;&nbsp;1.3 Notación](#toc_15456_1.3)\n",
    "- [2 Planteamiento del problema](#toc_15456_2)\n",
    "- [&nbsp;&nbsp;2.1 Matriz X que contiene nuestros ejemplos](#toc_15456_2.1)\n",
    "- [&nbsp;&nbsp;2.2 Vector de parámetros w, b](#toc_15456_2.2)\n",
    "- [3 Predicción del modelo con múltiples variables](#toc_15456_3)\n",
    "- [3.1 Predicción simple elemento por elemento](#toc_15456_3.1)\n",
    "- [&nbsp;&nbsp;3.2 Predicción única, vector](#toc_15456_3.2)\n",
    "- [4 Calcular el coste con múltiples variables](#toc_15456_4)\n",
    "- [5 Descenso de gradiente con múltiples variables](#toc_15456_5)\n",
    "- [&nbsp;&nbsp;5.1 Calcular Gradiente con Múltiples Variables](#toc_15456_5.1)\n",
    "- [5.2 Descenso del gradiente con múltiples variables](#toc_15456_5.2)\n",
    "- [6 Felicitaciones](#toc_15456_6)\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"><b>Texto original</b></font></summary>\n",
    "\n",
    "In this lab, you will extend the data structures and previously developed routines to support multiple features. Several routines are updated making the lab appear lengthy, but it makes minor adjustments to previous routines making it quick to review.\n",
    "# Outline\n",
    "- [&nbsp;&nbsp;1.1 Goals](#toc_15456_1.1)\n",
    "- [&nbsp;&nbsp;1.2 Tools](#toc_15456_1.2)\n",
    "- [&nbsp;&nbsp;1.3 Notation](#toc_15456_1.3)\n",
    "- [2 Problem Statement](#toc_15456_2)\n",
    "- [&nbsp;&nbsp;2.1 Matrix X containing our examples](#toc_15456_2.1)\n",
    "- [&nbsp;&nbsp;2.2 Parameter vector w, b](#toc_15456_2.2)\n",
    "- [3 Model Prediction With Multiple Variables](#toc_15456_3)\n",
    "- [&nbsp;&nbsp;3.1 Single Prediction element by element](#toc_15456_3.1)\n",
    "- [&nbsp;&nbsp;3.2 Single Prediction, vector](#toc_15456_3.2)\n",
    "- [4 Compute Cost With Multiple Variables](#toc_15456_4)\n",
    "- [5 Gradient Descent With Multiple Variables](#toc_15456_5)\n",
    "- [&nbsp;&nbsp;5.1 Compute Gradient with Multiple Variables](#toc_15456_5.1)\n",
    "- [&nbsp;&nbsp;5.2 Gradient Descent With Multiple Variables](#toc_15456_5.2)\n",
    "- [6 Congratulations](#toc_15456_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.1\"></a>\n",
    "## 1.1 Objetivos\n",
    "- Extender nuestras rutinas de modelos de regresión para soportar múltiples características\n",
    "    - Extender las estructuras de datos para soportar múltiples características\n",
    "    - Reescribir las rutinas de predicción, coste y gradiente para soportar múltiples características\n",
    "    - Utilizar NumPy `np.dot` para vectorizar sus implementaciones para mayor velocidad y simplicidad\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"><b>Texto original</b></font></summary>\n",
    "\n",
    "## 1.1 Goals\n",
    "- Extend our regression model  routines to support multiple features\n",
    "    - Extend data structures to support multiple features\n",
    "    - Rewrite prediction, cost and gradient routines to support multiple features\n",
    "    - Utilize NumPy `np.dot` to vectorize their implementations for speed and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.2\"></a>\n",
    "## 1.2 Herramientas\n",
    "En este laboratorio, haremos uso de \n",
    "- NumPy, una popular biblioteca para la computación científica\n",
    "- Matplotlib, una biblioteca popular para trazar datos\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"><b>Texto original</b></font></summary>\n",
    "\n",
    "## 1.2 Tools\n",
    "In this lab, we will make use of: \n",
    "- NumPy, a popular library for scientific computing\n",
    "- Matplotlib, a popular library for plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.3\"></a>\n",
    "## 1.3 Notacion\n",
    "A continuación se presenta un resumen de algunas de las anotaciones que encontrará, actualizadas para múltiples funciones.  \n",
    "\n",
    "|General | Description| Python (if applicable) |\n",
    "|:--------------|:------------------|:----------------------:|\n",
    "| $a$           | Escalar, normal  ||\n",
    "| $\\mathbf{A}$  | Matriz, Mayuscula negrita                              ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | Matriz ejemplos de entrenamiento   | `X_train` |   \n",
    "|  $\\mathbf{y}$  | Vector con valores exactos        | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$ Enecimo ejemplo| `X[i]`, `y[i]`|\n",
    "| m | Numero de ejemplos | `m`|\n",
    "| n | Número de características en cada ejemplo | `n`|\n",
    "|  $\\mathbf{w}$  |  Parametro: weight (pesos),                       | `w`    |\n",
    "|  $b$           |  Parametro : bias (sesgo)                         | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | El resultado de la evaluación del modelo en $\\mathbf{x^{(i)}}$ parametrizado por $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "\n",
    "\n",
    "## 1.3 Notation\n",
    "Here is a summary of some of the notation you will encounter, updated for multiple features.  \n",
    "\n",
    "|General | Description| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example maxtrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2\"></a>\n",
    "2 Planteamiento del problema\n",
    "\n",
    "Se utilizará el ejemplo motivador de la predicción del precio de la vivienda. El conjunto de datos de entrenamiento contiene tres ejemplos con cuatro características (tamaño, dormitorios, pisos y, edad) que se muestran en la tabla siguiente.  Tenga en cuenta que, a diferencia de los laboratorios anteriores, el tamaño está expresado en pies cuadrados en lugar de 1000 pies cuadrados. Esto causa un problema, que se resolverá en el próximo laboratorio.\n",
    "\n",
    "| Tamaño (pies cuadrados)| número de dormitorios| número de plantas| edad de la vivienda| precio (miles de dólares).|\n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |\n",
    "\n",
    "Construirás un modelo de regresión lineal utilizando estos valores para poder predecir el precio de otras casas. Por ejemplo, una casa de 1200 pies cuadrados, 3 dormitorios, 1 piso, 40 años de antigüedad.  \n",
    "\n",
    "Por favor, ejecute la siguiente celda de código para crear sus variables `X_train` y `y_train`.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "\n",
    "# 2 Problem Statement\n",
    "\n",
    "You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n",
    "\n",
    "Please run the following code cell to create your `X_train` and `y_train` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.1\"></a>\n",
    "## 2.1 Matriz X que contiene nuestros ejemplos\n",
    "Al igual que la tabla anterior, los ejemplos se almacenan en una matriz NumPy `X_train`. Cada fila de la matriz representa un ejemplo. Cuando usted tiene $m$ ejemplos de entrenamiento ( $m$ es tres en nuestro ejemplo), y hay $n$ características (cuatro en nuestro ejemplo), $\\mathbf{X}$ es una matriz con dimensiones ($m$, $n$) (m filas, n columnas).\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Notación:\n",
    "- $\\mathbf{x}^{(i)}$ es el vector que contiene el ejemplo i.  $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ es el elemento j en el ejemplo i. El superíndice entre paréntesis indica el número del ejemplo mientras que el subíndice representa un elemento.  \n",
    "\n",
    "Mostrar los datos de entrada.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "\n",
    "## 2.1 Matrix X containing our examples\n",
    "Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.  \n",
    "\n",
    "Display the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (3, 4), X Type:<class 'numpy.ndarray'>)\n",
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "y Shape: (3,), y Type:<class 'numpy.ndarray'>)\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(X_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.2\"></a>\n",
    "## 2.2 Vector de parámetros w, b\n",
    "\n",
    "* $\\mathbf{w}$ es un vector con $n$ elementos.\n",
    "  - Cada elemento contiene el parámetro asociado a una característica.\n",
    "  - En nuestro conjunto de datos, n es 4.\n",
    "  - nocionalmente, lo dibujamos como un vector de columnas\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ es un escalar. \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "\n",
    "## 2.2 Parameter vector w, b\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - in our dataset, n is 4.\n",
    "  - notionally, we draw this as a column vector\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la demostración, $\\mathbf{w}$ y $b$ serán cargados con algunos valores iniciales seleccionados que están cerca del óptimo. $\\mathbf{w}$ es un vector NumPy 1-D.\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "For demonstration, $\\mathbf{w}$ and $b$ will be loaded with some initial selected values that are near the optimal. $\\mathbf{w}$ is a 1-D NumPy vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "# 3 Predicción del modelo con múltiples variables\n",
    "La predicción del modelo con múltiples variables viene dada por el modelo lineal:\n",
    "\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "donde $\\cdot$ es el producto vectorial `dot product` \n",
    "\n",
    "Para demostrar el producto punto, implementaremos la predicción utilizando (1) y (2).\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "\n",
    "# 3 Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.1\"></a>\n",
    "## 3.1 Predicción única elemento por elemento\n",
    "Nuestra predicción anterior multiplicaba el valor de una característica por un parámetro y añadía un parámetro de sesgo. Una extensión directa de nuestra implementación anterior de la predicción a múltiples características sería implementar (1) arriba usando un bucle sobre cada elemento, realizando la multiplicación con su parámetro y luego añadiendo el parámetro de sesgo al final.\n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "\n",
    "## 3.1 Single Prediction element by element\n",
    "Our previous prediction multiplied one feature value by one parameter and added a bias parameter. A direct extension of our previous implementation of prediction to multiple features would be to implement (1) above using loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]  \n",
    "        p = p + p_i         \n",
    "    p = p + b                \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa la forma de `x_vec`. Es un vector NumPy 1-D con 4 elementos, (4,). El resultado, `f_wb` es un escalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.2\"></a>\n",
    "\n",
    "## 3.2 Predicción única, vector\n",
    "\n",
    "Observando que la ecuación (1) anterior puede implementarse utilizando el producto punto como en (2) anterior. Podemos hacer uso de las operaciones vectoriales para acelerar las predicciones.\n",
    "\n",
    "Recordemos del laboratorio de Python/Numpy que NumPy `np.dot()`[[link](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)] puede utilizarse para realizar un producto punto vectorial. \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "\n",
    "## 3.2 Single Prediction, vector\n",
    "\n",
    "Noting that equation (1) above can be implemented using the dot product as in (2) above. We can make use of vector operations to speed up predictions.\n",
    "\n",
    "Recall from the Python/Numpy lab that NumPy `np.dot()`[[link](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)] can be used to perform a vector dot product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados y las formas son los mismos que en la versión anterior, que utilizaba el bucle. En adelante, se utilizará `np.dot` para estas operaciones. La predicción es ahora una única sentencia. La mayoría de las rutinas la implementarán directamente en lugar de llamar a una rutina de predicción separada.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "\n",
    "The results and shapes are the same as the previous version which used looping. Going forward, `np.dot` will be used for these operations. The prediction is now a single statement. Most routines will implement it directly rather than calling a separate predict routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# 4 Calcular el coste con múltiples variables\n",
    "\n",
    "La ecuación para la función de coste con múltiples variables $J(\\mathbf{w},b)$ es:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "donde:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "A diferencia de los laboratorios anteriores, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ son vectores en lugar de escalares que soportan múltiples características.\n",
    "\n",
    "A continuación se muestra una implementación de las ecuaciones (3) y (4). Tenga en cuenta que esto utiliza un *patrón estándar para este curso* donde se utiliza un bucle for sobre todos los ejemplos `m`.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "\n",
    "# 4 Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features.\n",
    "\n",
    "Below is an implementation of equations (3) and (4). Note that this uses a *standard pattern for this course* where a for loop over all `m` examples is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904428966628e-12\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**: Cost at optimal w : 1.5578904045996674e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "\n",
    "$$\\begin{align*} \\text{repite}&\\text{ hasta la convergencia:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "donde , n es el número de características, los parámetros $w_j$, $b$, se actualizan simultáneamente y donde \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* m es el número de ejemplos de entrenamiento en el conjunto de datos\n",
    "\n",
    "    \n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$  es la predicción del modelo, mientras que $y^{(i)}$ es el valor objetivo\n",
    "\n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "\n",
    "# 5 Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{7}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{8}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.1\"></a>\n",
    "## 5.1 Calcular el gradiente con múltiples variables\n",
    "A continuación se presenta una implementación para calcular las ecuaciones (6) y (7). Hay muchas maneras de implementar esto. En esta versión, hay un\n",
    "- bucle externo sobre todos los m ejemplos. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ para el ejemplo puede ser calculado directamente y acumulado\n",
    "    - en un segundo bucle sobre todas las n características:\n",
    "        - $\\frac{parcial J(\\mathbf{w},b)}{parcial w_j}$ se calcula para cada $w_j$.\n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "## 5.1 Compute Gradient with Multiple Variables\n",
    "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot( X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: -1.6739251501955248e-06\n",
      "dj_dw at initial w,b: \n",
      " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n"
     ]
    }
   ],
   "source": [
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**:   \n",
    "dj_db at initial w,b: -1.6739251122999121e-06  \n",
    "dj_dw at initial w,b:   \n",
    " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.2\"></a>\n",
    "## 5.2 Descenso de gradiente con múltiples variables\n",
    "La siguiente rutina implementa la ecuación (5) anterior.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"green\"<b>Texto original</b></font></summary>\n",
    "## 5.2 Gradient Descent With Multiple Variables\n",
    "The routine below implements equation (5) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # Una matriz para almacenar los costes J y w en cada iteración, principalmente para graficarlos posteriormente\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  # avoid modifying global w within function  ??\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calcular el gradiente y actualizar los parámetros\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Actualizar los parámetros mediante w, b, alfa y gradiente\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you will test the implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**:    \n",
    "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]   \n",
    "prediction: 426.19, target value: 460  \n",
    "prediction: 286.17, target value: 232  \n",
    "prediction: 171.47, target value: 178  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist)\n",
    "ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These results are not inspiring*! Cost is still declining and our predictions are not very accurate. The next lab will explore how to improve on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"toc_15456_6\"></a>\n",
    "# 6 Congratulations!\n",
    "In this lab you:\n",
    "- Redeveloped the routines for linear regression, now with multiple variables.\n",
    "- Utilized NumPy `np.dot` to vectorize the implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "15456"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
