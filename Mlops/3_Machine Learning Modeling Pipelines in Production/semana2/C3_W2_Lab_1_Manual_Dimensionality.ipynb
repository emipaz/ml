{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oaTnIQF9V9m"
      },
      "source": [
        "# Laboratorio no calificado: Ingeniería de rasgos manuales\n",
        "------------------------\n",
        " \n",
        "Bienvenido, durante este laboratorio no graduado vas a realizar ingeniería de características usando TensorFlow y Keras. Teniendo un conocimiento más profundo del problema que estás tratando y proponiendo transformaciones a las características en bruto verás cómo aumenta el poder predictivo de tu modelo. En concreto harás:\n",
        "\n",
        "\n",
        "1. Definir el modelo usando columnas de características.\n",
        "2. Utilizar capas lambda para realizar ingeniería de características sobre algunas de ellas.\n",
        "3. 3. Comparar el historial de entrenamiento y las predicciones del modelo antes y después de la ingeniería de características.\n",
        "\n",
        "**Nota**: Este laboratorio tiene algunos ajustes en comparación con el código que acaba de ver en las conferencias. El principal es que las variables relacionadas con el tiempo no se utilizan en el modelo de ingeniería de características.\n",
        "\n",
        "Empecemos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P7VGW6w9V9n"
      },
      "source": [
        "First, install and import the necessary packages, set up paths to work on and download the dataset.\n",
        "\n",
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJlL4SaV9V9v"
      },
      "source": [
        "# Import the packages\n",
        "\n",
        "# Utilities\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# For visualization\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# For modelling\n",
        "import tensorflow as tf\n",
        "from tensorflow import feature_column as fc\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Set TF logger to only print errors (dismiss warnings)\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvhL6GAI9V9z"
      },
      "source": [
        "## Cargar conjunto de datos taxifare\n",
        "\n",
        "Para este laboratorio se va a utilizar una versión modificada del conjunto de datos [Taxi Fare dataset](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data), que se ha preprocesado y dividido previamente. \n",
        "\n",
        "En primer lugar, cree el directorio en el que se guardarán los datos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCvWVB4H9V90"
      },
      "source": [
        "if not os.path.isdir(\"/tmp/data\"):\n",
        "    os.makedirs(\"/tmp/data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC5BEPLxm2uP"
      },
      "source": [
        "descarga ahora los datos en formato `csv` desde un cubo de almacenamiento en la nube."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKH_WXZH9V94"
      },
      "source": [
        "!gsutil cp gs://cloud-training-demos/feat_eng/data/taxi*.csv /tmp/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIEJqz359V96"
      },
      "source": [
        "Let's check that the files were copied correctly and look like we expect them to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW6Sm4799V97"
      },
      "source": [
        "!ls -l /tmp/data/*.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EStEEdDsnXHW"
      },
      "source": [
        "Everything looks fine. Notice that there are three files, one for each split of `training`, `testing` and `validation`.\n",
        "\n",
        "## Inspect tha data\n",
        "\n",
        "Now take a look at the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bUQlBaCCfSD"
      },
      "source": [
        "pd.read_csv('/tmp/data/taxi-train.csv').head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ybIPxDnGTS"
      },
      "source": [
        "Los datos contienen un total de 8 variables.\n",
        "\n",
        "El `fare_amount` es el objetivo, el valor continuo que vamos a entrenar un modelo para predecir. Esto nos deja con 7 características. \n",
        "\n",
        "Sin embargo, este laboratorio se va a centrar en la transformación de los geoespaciales por lo que las características de tiempo `hourofday` y `dayofweek` serán ignorados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcIlWn3H9V-E"
      },
      "source": [
        "## Crear un pipeline de entrada \n",
        "\n",
        "Para cargar los datos para el modelo vas a utilizar una característica experimental de Tensorflow que permite cargar directamente desde un archivo `csv`.\n",
        "\n",
        "Para ello es necesario definir algunas listas que contengan información relevante del conjunto de datos, como el tipo de las columnas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxC3Qjx19V-E"
      },
      "source": [
        "# Especifique qué columna es el objetivo\n",
        "LABEL_COLUMN = 'fare_amount'\n",
        "\n",
        "# Especificar columnas numéricas\n",
        "# Tenga en cuenta que debería crear otra lista con STRING_COLS si \n",
        "# tuviera datos de texto pero en este caso todas las características son numéricas\n",
        "NUMERIC_COLS = ['pickup_longitude', 'pickup_latitude',\n",
        "                'dropoff_longitude', 'dropoff_latitude',\n",
        "                'passenger_count', 'hourofday', 'dayofweek']\n",
        "\n",
        "\n",
        "# Una función para separar características y etiquetas\n",
        "def features_and_labels(row_data):\n",
        "    label = row_data.pop(LABEL_COLUMN)\n",
        "    return row_data, label\n",
        "\n",
        "\n",
        "# Un método de utilidad para crear un conjunto de datos tf.data a partir de un archivo CSV\n",
        "def load_dataset(pattern, batch_size=1, mode='eval'):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(pattern, batch_size)\n",
        "    \n",
        "    dataset = dataset.map(features_and_labels)  # features, label\n",
        "    if mode == 'train':\n",
        "        # Observe que se utiliza el método de repetición para que este conjunto de datos se repita infinitamente.\n",
        "        dataset = dataset.shuffle(1000).repeat()\n",
        "        # aprovechar el multihilo; 1=AUTOTUNE\n",
        "        dataset = dataset.prefetch(1)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAhDWtck9V-K"
      },
      "source": [
        "## Crear un Modelo DNN en Keras\n",
        "\n",
        "Ahora construirás una Red Neuronal simple con las características numéricas como entrada representadas por una capa [`DenseFeatures`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/DenseFeatures) (que produce un Tensor denso basado en las características dadas), dos capas densas con funciones de activación ReLU y una capa de salida con una función de activación lineal (ya que este es un problema de regresión).\n",
        "\n",
        "Dado que el modelo se define utilizando \"columnas de características\", la primera capa puede tener un aspecto diferente al habitual. Esto se hace declarando dos diccionarios, uno para las entradas (definidas como capas de entrada) y otro para las características (definidas como columnas de características).\n",
        "\n",
        "Luego se calcula el tensor `DenseFeatures` pasando las columnas de características al constructor de la capa `DenseFeatures` y pasando las entradas al tensor resultante (esto es más fácil de entender con código):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jheXumL9V-K"
      },
      "source": [
        "def build_dnn_model():\n",
        "    # input layer\n",
        "    inputs = {\n",
        "        colname: layers.Input(name=colname, shape=(), dtype='float32')\n",
        "        for colname in NUMERIC_COLS\n",
        "    }\n",
        "\n",
        "    # feature_columns\n",
        "    feature_columns = {\n",
        "        colname: fc.numeric_column(colname)\n",
        "        for colname in NUMERIC_COLS\n",
        "    }\n",
        "\n",
        "    # Constructor para DenseFeatures toma una lista de columnas numéricas\n",
        "    # y el tensor resultante toma un diccionario de capas Input\n",
        "    dnn_inputs = layers.DenseFeatures(feature_columns.values())(inputs)\n",
        "\n",
        "    # dos capas ocultas de 32 y 8 unidades, respectivamente\n",
        "    h1 = layers.Dense(32, activation='relu', name='h1')(dnn_inputs)\n",
        "    h2 = layers.Dense(8, activation='relu', name='h2')(h1)\n",
        "\n",
        "    # la salida final es una activación lineal porque este es un problema de regresión\n",
        "    output = layers.Dense(1, activation='linear', name='fare')(h2)\n",
        "\n",
        "    # Create model with inputs and output\n",
        "    model = models.Model(inputs, output)\n",
        "\n",
        "    # compile model (Mean Squared Error is suitable for regression)\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='mse', \n",
        "                  metrics=[\n",
        "                      tf.keras.metrics.RootMeanSquaredError(name='rmse'), \n",
        "                      'mse'\n",
        "                  ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuKLnld69V-N"
      },
      "source": [
        "We'll build our DNN model and inspect the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnvtEUnd9V-N"
      },
      "source": [
        "# Save compiled model into a variable\n",
        "model = build_dnn_model()\n",
        "\n",
        "# Plot the layer architecture and relationship between input features\n",
        "tf.keras.utils.plot_model(model, 'dnn_model.png', show_shapes=False, rankdir='LR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol9FVnFnayeU"
      },
      "source": [
        "With the model architecture defined it is time to train it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5R1zEJj9V-Q"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "You are going to train the model for 20 epochs using a batch size of 32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwW7dL8t9V-Q"
      },
      "source": [
        "NUM_EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 32 \n",
        "NUM_TRAIN_EXAMPLES = len(pd.read_csv('/tmp/data/taxi-train.csv'))\n",
        "NUM_EVAL_EXAMPLES = len(pd.read_csv('/tmp/data/taxi-valid.csv'))\n",
        "\n",
        "print(f\"training split has {NUM_TRAIN_EXAMPLES} examples\\n\")\n",
        "print(f\"evaluation split has {NUM_EVAL_EXAMPLES} examples\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D_sr3UQfpGj"
      },
      "source": [
        "Use the previously defined function to load the datasets from the original csv files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWwcyV6I9V-S"
      },
      "source": [
        "# Training dataset\n",
        "trainds = load_dataset('/tmp/data/taxi-train*', TRAIN_BATCH_SIZE, 'train')\n",
        "\n",
        "# Evaluation dataset\n",
        "evalds = load_dataset('/tmp/data/taxi-valid*', 1000, 'eval').take(NUM_EVAL_EXAMPLES//1000)\n",
        "\n",
        "# Necesita ser especificado ya que el conjunto de datos es infinito \n",
        "# Esto ocurre porque se utilizó el método de repetición al crear el conjunto de datos\n",
        "steps_per_epoch = NUM_TRAIN_EXAMPLES // TRAIN_BATCH_SIZE\n",
        "\n",
        "# Train the model and save the history\n",
        "history = model.fit(trainds,\n",
        "                    validation_data=evalds,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    steps_per_epoch=steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbcJqYFu9V-V"
      },
      "source": [
        "### Visualize training curves\n",
        "\n",
        "Now lets visualize the training history of the model with the raw features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVlOutPY9V-V"
      },
      "source": [
        "# Function for plotting metrics for a given history\n",
        "def plot_curves(history, metrics):\n",
        "    nrows = 1\n",
        "    ncols = 2\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "    for idx, key in enumerate(metrics):  \n",
        "        ax = fig.add_subplot(nrows, ncols, idx+1)\n",
        "        plt.plot(history.history[key])\n",
        "        plt.plot(history.history[f'val_{key}'])\n",
        "        plt.title(f'model {key}')\n",
        "        plt.ylabel(key)\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "\n",
        "# Plot history metrics\n",
        "plot_curves(history, ['loss', 'mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUpJkrpNhm8-"
      },
      "source": [
        "El historial de entrenamiento no parece muy prometedor y muestra un comportamiento errático. Parece que el proceso de entrenamiento tuvo dificultades para atravesar el espacio de alta dimensión que crean las características actuales. \n",
        "\n",
        "No obstante, utilicémoslo para la predicción.\n",
        "\n",
        "Obsérvese que los valores de latitud y longitud deberían girar en torno a (`37`, `45`) y (`-70`, `-78`) respectivamente, ya que son el rango de coordenadas de la ciudad de Nueva York."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtNMWIg69V-a"
      },
      "source": [
        "# Define a taxi ride (a data point)\n",
        "taxi_ride = {\n",
        "    'pickup_longitude': tf.convert_to_tensor([-73.982683]),\n",
        "    'pickup_latitude': tf.convert_to_tensor([40.742104]),\n",
        "    'dropoff_longitude': tf.convert_to_tensor([-73.983766]),\n",
        "    'dropoff_latitude': tf.convert_to_tensor([40.755174]),\n",
        "    'passenger_count': tf.convert_to_tensor([3.0]),\n",
        "    'hourofday': tf.convert_to_tensor([3.0]),\n",
        "    'dayofweek': tf.convert_to_tensor([3.0]),\n",
        "}\n",
        "\n",
        "# Use the model to predict\n",
        "prediction = model.predict(taxi_ride, steps=1)\n",
        "\n",
        "# Print prediction\n",
        "print(f\"the model predicted a fare total of {float(prediction):.2f} USD for the ride.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZhFMdh7ixN6"
      },
      "source": [
        "El modelo predijo que este paseo en particular rondaría los 12 USD. Sin embargo, usted sabe que el rendimiento del modelo no es el mejor, como lo demuestra el historial de entrenamiento. Mejorémoslo utilizando **Ingeniería de características**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1TNR8nI3Bdv"
      },
      "source": [
        "## Mejorar el rendimiento del modelo mediante la ingeniería de rasgos \n",
        "\n",
        "En lo sucesivo, sólo se utilizarán características geoespaciales, ya que son las más relevantes a la hora de calcular la tarifa, puesto que este valor depende sobre todo de la distancia recorrida:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRSTOWC7285u"
      },
      "source": [
        "# Drop dayofweek and hourofday features\n",
        "NUMERIC_COLS = ['pickup_longitude', 'pickup_latitude',\n",
        "                'dropoff_longitude', 'dropoff_latitude']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm5rY2B_9V-d"
      },
      "source": [
        "Dado que se trata exclusivamente de datos geoespaciales, se crearán algunas transformaciones que tengan en cuenta esta naturaleza geoespacial. Esto ayuda al modelo a hacer una mejor representación del problema en cuestión.\n",
        "\n",
        "Por ejemplo, el modelo no puede entender mágicamente lo que se supone que representa una coordenada y, dado que los datos proceden únicamente de Nueva York, la latitud y la longitud giran en torno a (`37`, `45`) y (`-70`, `-78`) respectivamente, lo cual es arbitrario para el modelo. Un buen primer paso es escalar estos valores. \n",
        "\n",
        "**Nótese que todas las transformaciones se crean definiendo funciones**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dOFcJJn9V-j"
      },
      "source": [
        "def scale_longitude(lon_column):\n",
        "    return (lon_column + 78)/8."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGLNozez9V-m"
      },
      "source": [
        "def scale_latitude(lat_column):\n",
        "    return (lat_column - 37)/8."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhMqPABOm4h1"
      },
      "source": [
        "Otro dato importante es que la tarifa de un viaje en taxi es proporcional a la distancia del trayecto. Sin embargo, tal y como están configuradas actualmente las características, el modelo no puede deducir que el par (`latitud_de_recogida`, `longitud_de_recogida`) representa el punto en el que el pasajero inició el trayecto y el par (`latitud_de_salida`, `longitud_de_salida`) representa el punto en el que finalizó el trayecto. Y lo que es más importante, el modelo no es consciente de que la distancia entre estos dos puntos es crucial para predecir la tarifa.\n",
        "\n",
        "Para solucionar esto, se necesita una nueva característica (que es una transformación de las otras) que proporcione esta información."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-_yQj1S9V-g"
      },
      "source": [
        "def euclidean(params):\n",
        "    lon1, lat1, lon2, lat2 = params\n",
        "    londiff = lon2 - lon1\n",
        "    latdiff = lat2 - lat1\n",
        "    return tf.sqrt(londiff*londiff + latdiff*latdiff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss4nvMbP9V-o"
      },
      "source": [
        "### Aplicar transformaciones\n",
        "\n",
        "Ahora definiremos la función `transform` que aplicará las funciones de transformación previamente definidas. Para aplicar las transformaciones reales que va a utilizar capas `Lambda` aplicar una función a los valores (en este caso las entradas).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kXOft7O9V-o"
      },
      "source": [
        "def transform(inputs, numeric_cols):\n",
        "\n",
        "    # Make a copy of the inputs to apply the transformations to\n",
        "    transformed = inputs.copy()\n",
        "\n",
        "    # Define feature columns\n",
        "    feature_columns = {\n",
        "        colname: tf.feature_column.numeric_column(colname)\n",
        "        for colname in numeric_cols\n",
        "    }\n",
        "\n",
        "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
        "    for lon_col in ['pickup_longitude', 'dropoff_longitude']:\n",
        "        transformed[lon_col] = layers.Lambda(\n",
        "            scale_longitude,\n",
        "            name=f\"scale_{lon_col}\")(inputs[lon_col])\n",
        "\n",
        "    # Scaling latitude from range [37, 45] to [0, 1]\n",
        "    for lat_col in ['pickup_latitude', 'dropoff_latitude']:\n",
        "        transformed[lat_col] = layers.Lambda(\n",
        "            scale_latitude,\n",
        "            name=f'scale_{lat_col}')(inputs[lat_col])\n",
        "\n",
        "    # add Euclidean distance\n",
        "    transformed['euclidean'] = layers.Lambda(\n",
        "        euclidean,\n",
        "        name='euclidean')([inputs['pickup_longitude'],\n",
        "                           inputs['pickup_latitude'],\n",
        "                           inputs['dropoff_longitude'],\n",
        "                           inputs['dropoff_latitude']])\n",
        "        \n",
        "    \n",
        "    # Add euclidean distance to feature columns\n",
        "    feature_columns['euclidean'] = fc.numeric_column('euclidean')\n",
        "\n",
        "    return transformed, feature_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5HQT79B9V-s"
      },
      "source": [
        "## Update the model\n",
        "\n",
        "Next, you'll create the DNN model now with the engineered (transformed) features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULaSC25j9V-s"
      },
      "source": [
        "def build_dnn_model():\n",
        "    \n",
        "    # input layer (notice type of float32 since features are numeric)\n",
        "    inputs = {\n",
        "        colname: layers.Input(name=colname, shape=(), dtype='float32')\n",
        "        for colname in NUMERIC_COLS\n",
        "    }\n",
        "\n",
        "    # transformed features\n",
        "    transformed, feature_columns = transform(inputs, numeric_cols=NUMERIC_COLS)\n",
        "\n",
        "    # Constructor for DenseFeatures takes a list of numeric columns\n",
        "    # and the resulting tensor takes a dictionary of Lambda layers\n",
        "    dnn_inputs = layers.DenseFeatures(feature_columns.values())(transformed)\n",
        "\n",
        "    # two hidden layers of 32 and 8 units, respectively\n",
        "    h1 = layers.Dense(32, activation='relu', name='h1')(dnn_inputs)\n",
        "    h2 = layers.Dense(8, activation='relu', name='h2')(h1)\n",
        "\n",
        "    # final output is a linear activation because this is a regression problem\n",
        "    output = layers.Dense(1, activation='linear', name='fare')(h2)\n",
        "\n",
        "    # Create model with inputs and output\n",
        "    model = models.Model(inputs, output)\n",
        "\n",
        "    # Compile model (Mean Squared Error is suitable for regression)\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='mse', \n",
        "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mse'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyNi1qgB9V-v"
      },
      "source": [
        "# Save compiled model into a variable\n",
        "model = build_dnn_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI9UTFc29V-x"
      },
      "source": [
        "Let's see how the model architecture has changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vtdSjyJ9V-y"
      },
      "source": [
        "# Plot the layer architecture and relationship between input features\n",
        "tf.keras.utils.plot_model(model, 'dnn_model_engineered.png', show_shapes=False, rankdir='LR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQqxaAL-3X-V"
      },
      "source": [
        "Este gráfico es muy útil para comprender las relaciones y dependencias entre las características originales y las transformadas.\n",
        "\n",
        "**Observe que la entrada del modelo consta ahora de 5 características en lugar de las 7 originales, reduciendo así la dimensionalidad del problema.\n",
        "\n",
        "Entrenemos ahora el modelo que incluye la ingeniería de características."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_rE0U9W9V-0"
      },
      "source": [
        "history = model.fit(trainds,\n",
        "                    validation_data=evalds,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    steps_per_epoch=steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vjGcFRx9V-2"
      },
      "source": [
        "Observe que las características `passenger_count`, `hourofday` y `dayofweek` se excluyeron porque se omitieron al definir el canal de entrada.\n",
        "\n",
        "Ahora vamos a visualizar el historial de entrenamiento del modelo con las características diseñadas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfacl2gP9V-2"
      },
      "source": [
        "# Plot history metrics\n",
        "plot_curves(history, ['loss', 'mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV8ZGZev9V-5"
      },
      "source": [
        "Esto parece mucho mejor que el historial de entrenamiento anterior. Ahora las métricas de pérdida y error están disminuyendo con cada epoch y ambas curvas (entrenamiento y validación) están muy cerca la una de la otra. ¡Buen trabajo!\n",
        "\n",
        "Hagamos una predicción con este nuevo modelo sobre el ejemplo que utilizamos anteriormente. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Biqounrh9V-5"
      },
      "source": [
        "# Use the model to predict\n",
        "prediction = model.predict(taxi_ride, steps=1)\n",
        "\n",
        "# Print prediction\n",
        "print(f\"the model predicted a fare total of {float(prediction):.2f} USD for the ride.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVZCFMi74xrj"
      },
      "source": [
        "Vaya, ahora el modelo predice una tarifa que es aproximadamente la mitad de lo que predecía el modelo anterior. Parece que el modelo con las características brutas estaba sobreestimando la tarifa por un gran margen.\n",
        "\n",
        "Observe que aparece una advertencia, ya que el diccionario `taxi_ride` contiene información sobre las características no utilizadas. Puedes suprimirlo redefiniendo `taxi_ride` sin estos valores, pero es útil saber que Keras es lo suficientemente inteligente como para manejarlo por sí mismo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeMvthqfugsm"
      },
      "source": [
        "**Ahora debería tener una comprensión más clara de la importancia y el impacto de realizar ingeniería de características en sus datos. \n",
        "\n",
        "Este proceso es muy específico y requiere una gran comprensión de la situación que se está modelando. Por este motivo, se han desarrollado nuevas técnicas que pasan de la ingeniería manual a la automática, y podrás comprobar algunas de ellas en un próximo laboratorio.\n",
        "\n",
        "\n",
        "**Sigan así."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hbMGAR10DThI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}