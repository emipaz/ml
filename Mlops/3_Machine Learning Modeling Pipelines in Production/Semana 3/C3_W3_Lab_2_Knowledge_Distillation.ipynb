{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2hyuOxRwabt"
      },
      "source": [
        "# Laboratorio No Calificado: Destilación del conocimiento\n",
        "------------------------\n",
        " \n",
        "Bienvenido, durante este laboratorio no calificado vas a realizar una técnica de compresión de modelos conocida como **destilación de conocimiento** en la cual un modelo `alumno` \"aprende\" de un modelo más complejo conocido como `maestro`. En concreto\n",
        "\n",
        "\n",
        "1. Definir una clase `Distiller` con la lógica personalizada para el proceso de destilación.\n",
        "2. 2. Entrenar el modelo `teacher` que es una CNN que implementa la regularización a través del abandono.\n",
        "3. 3. Entrenar un modelo `student` (una versión más pequeña del profesor sin regularización) utilizando destilación de conocimiento.\n",
        "4. 4. Entrenar otro modelo `alumno` desde cero sin destilación llamado `alumno_scratch`.\n",
        "5. Compare los tres estudiantes.\n",
        "\n",
        "\n",
        "Este cuaderno está basado en [this](https://keras.io/examples/vision/knowledge_distillation/) tutorial oficial de Keras. \n",
        "\n",
        "Si quieres una aproximación más teórica a este tema no dejes de consultar este paper [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531). \n",
        "\n",
        "¡Vamos a empezar!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAhJX9iLwabu"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SosaPG6jwabv"
      },
      "source": [
        "# For setting random seeds\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(42)\n",
        "\n",
        "# Libraries\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# More random seed setup\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MsH7h6tqC2i"
      },
      "source": [
        "## Preparar los datos\n",
        "\n",
        "Para este laboratorio utilizarás el archivo [cats vs dogs](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs) que está compuesto por muchas imágenes de gatos y perros junto a sus respectivas etiquetas. \n",
        "\n",
        "Comienza descargando los datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGWF89iLwab0"
      },
      "source": [
        "# Define train/test splits\n",
        "splits = ['train[:80%]', 'train[80%:90%]', 'train[90%:]']\n",
        "\n",
        "# Download the dataset\n",
        "(train_examples, validation_examples, test_examples), info = tfds.load('cats_vs_dogs', with_info=True, as_supervised=True, split=splits)\n",
        "\n",
        "# Print useful information\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes\n",
        "\n",
        "print(f\"There are {num_examples} images for {num_classes} classes.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LIucSJ8rKAG"
      },
      "source": [
        "Preprocesar los datos para el entrenamiento normalizando los valores de los píxeles, dándoles nueva forma y creando lotes de datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKhoLUfIR81q"
      },
      "source": [
        "# Some global variables\n",
        "pixels = 224\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Apply resizing and pixel normalization\n",
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
        "    return  image, label\n",
        "\n",
        "# Create batches of data\n",
        "train_batches      = train_examples.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "test_batches       = test_examples.map(format_image).batch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb5TgrJbZjJR"
      },
      "source": [
        "## Codificar el modelo personalizado `Distiller`\n",
        "\n",
        "Para implementar el proceso de destilación crearemos un modelo Keras personalizado al que llamaremos `Distiller`. Para ello necesitarás sobreescribir algunos de los métodos vanilla de un `keras.Model` para incluir la lógica personalizada para la destilación de conocimiento. Necesitas sobreescribir estos métodos:\n",
        "- `compile`:: Este modelo necesita algunos parámetros extra para ser compilado como las pérdidas del profesor y del alumno, el alfa y la temperatura.\n",
        "- `train_step`: Controla cómo se entrena el modelo. Aquí será donde se encuentre la lógica real de destilación del conocimiento. Este método es lo que se llama cuando se hace `model.fit`.\n",
        "- `test_step`:: Controla la evaluación del modelo. Este método es el que se utiliza cuando se ejecuta `model.evaluate`.\n",
        "\n",
        "Para obtener más información sobre la personalización de los modelos echa un vistazo a la [docs oficial](https://keras.io/guides/customizing_what_happens_in_fit/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdZ7JiqEwabw"
      },
      "source": [
        "class Distiller(keras.Model):\n",
        "\n",
        "  # Necesita los modelos del alumno y del profesor para crear una instancia de esta clase\n",
        "  def __init__(self, student, teacher):\n",
        "      super(Distiller, self).__init__()\n",
        "      self.teacher = teacher\n",
        "      self.student = student\n",
        "\n",
        "\n",
        "  # Se utilizará al llamar a model.compile()\n",
        "  def compile(self, optimizer, metrics, student_loss_fn,\n",
        "              distillation_loss_fn, alpha, temperature):\n",
        "\n",
        "      # Compilar utilizando el optimizador y las métricas\n",
        "      super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "      \n",
        "      # Añade los demás parámetros a la instancia\n",
        "      self.student_loss_fn = student_loss_fn\n",
        "      self.distillation_loss_fn = distillation_loss_fn\n",
        "      self.alpha = alpha\n",
        "      self.temperature = temperature\n",
        "\n",
        "\n",
        "  # Se utilizará al llamar a model.fit()\n",
        "  def train_step(self, data):\n",
        "      # Data is expected to be a tuple of (features, labels)\n",
        "      # Se espera que los datos sean una tupla de (características, etiquetas)\n",
        "      x, y = data\n",
        "\n",
        "      # Vanilla forward pass of the teacher\n",
        "      # Note that the teacher is NOT trained\n",
        "      # Tenga en cuenta que el maestro NO está entrenado\n",
        "      teacher_predictions = self.teacher(x, training = False)\n",
        "\n",
        "      # Use GradientTape to save gradients\n",
        "      with tf.GradientTape() as tape:\n",
        "          # Vanilla forward pass of the student\n",
        "          student_predictions = self.student(x, training = True)\n",
        "\n",
        "          # calcular la pérdida del alumno vainilla\n",
        "          student_loss = self.student_loss_fn(y, student_predictions)\n",
        "          \n",
        "          # Calcular la pérdida de destilación\n",
        "          # Debería ser la divergencia KL entre logits suavizada por un factor de temperatura\n",
        "          distillation_loss = self.distillation_loss_fn(\n",
        "              tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "              tf.nn.softmax(student_predictions / self.temperature, axis=1))\n",
        "\n",
        "          # Calcula la pérdida ponderando las dos pérdidas anteriores utilizando el parámetro alfa\n",
        "          loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "      # Utilizar tape.gradient para calcular gradientes para el alumno\n",
        "      trainable_vars = self.student.trainable_variables\n",
        "      gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "      # Update student weights \n",
        "      # Note that this done ONLY for the student # Actualizar los pesos del estudiante \n",
        "      # Tenga en cuenta que esto sólo se hace para el estudiante\n",
        "      self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "      # Update the metrics\n",
        "      self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "      # Devuelve un diccionario de rendimiento\n",
        "      # Verás que esto se emite durante el entrenamiento\n",
        "      results = {m.name: m.result() for m in self.metrics}\n",
        "      results.update({\"student_loss\": student_loss, \"distillation_loss\": distillation_loss})\n",
        "      return results\n",
        "\n",
        "\n",
        "  # Se utilizará al llamar a model.evaluate()\n",
        "  def test_step(self, data):\n",
        "      # Data is expected to be a tuple of (features, labels)\n",
        "      x, y = data\n",
        "\n",
        "      # Use student to make predictions\n",
        "      # Notice that the training param is set to False\n",
        "      y_prediction = self.student(x, training=False)\n",
        "\n",
        "      # Calculate student's vanilla loss\n",
        "      student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "      # Update the metrics\n",
        "      self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "      # Return a performance dictionary\n",
        "      # You will see this being outputted during inference\n",
        "      results = {m.name: m.result() for m in self.metrics}\n",
        "      results.update({\"student_loss\": student_loss})\n",
        "      return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1QXmNmisKNG"
      },
      "source": [
        "## Modelos de profesor y alumno\n",
        "\n",
        "Para los modelos se utilizará una arquitectura CNN estándar que implemente regularización a través de algunas capas dropout (en el caso del profesor), pero podría ser cualquier modelo de Keras. \n",
        "\n",
        "Define las funciones `create_model` para crear modelos con la arquitectura deseada utilizando el [Modelo Secuencial] de Keras (https://keras.io/guides/sequential_model/).\n",
        "\n",
        "Observa que `create_small_model` devuelve una versión simplificada del modelo (en cuanto a número de capas y ausencia de regularización) que devuelve `create_big_model`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35GyhKrgwt0o"
      },
      "source": [
        "# Teacher model\n",
        "def create_big_model():\n",
        "  tf.random.set_seed(42)\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(2)\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "# Student model\n",
        "def create_small_model():\n",
        "  tf.random.set_seed(42)\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(2)\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FsetAiyvHlr"
      },
      "source": [
        "Hay dos cosas importantes a tener en cuenta:\n",
        "- La última capa no tiene una activación softmax porque los logits brutos son necesarios para la destilación del conocimiento.\n",
        "- La regularización mediante capas de abandono se aplicará al profesor pero NO al alumno. Esto se debe a que el alumno debería ser capaz de aprender esta regularización a través del proceso de destilación.\n",
        "\n",
        "Recuerde que el modelo del alumno puede considerarse como una versión simplificada (o comprimida) del modelo del profesor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HazdkHp9j7Ur"
      },
      "source": [
        "# Create the teacher\n",
        "teacher = create_big_model()\n",
        "\n",
        "# Plot architecture\n",
        "keras.utils.plot_model(teacher, rankdir=\"LR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bywn32D7kZ9H"
      },
      "source": [
        "# Create the student\n",
        "student = create_small_model()\n",
        "\n",
        "# Plot architecture\n",
        "keras.utils.plot_model(student, rankdir=\"LR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lxJnnI4xs-s"
      },
      "source": [
        "Comprueba la diferencia real en el número de parámetros entrenables (pesos y sesgos) entre ambos modelos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed8Sd21vvwSK"
      },
      "source": [
        "# Calculates number of trainable params for a given model\n",
        "def num_trainable_params(model):\n",
        "  return np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
        "\n",
        "\n",
        "student_params = num_trainable_params(student)\n",
        "teacher_params = num_trainable_params(teacher)\n",
        "\n",
        "print(f\"Teacher model has: {teacher_params} trainable params.\\n\")\n",
        "print(f\"Student model has: {student_params} trainable params.\\n\")\n",
        "print(f\"Teacher model is roughly {teacher_params//student_params} times bigger than the student model.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_O66k7dwab1"
      },
      "source": [
        "### Entrenar al profesor\n",
        "\n",
        "En la destilación de conocimiento se asume que el profesor ya ha sido entrenado, por lo que el primer paso natural es entrenar al profesor. Lo hará durante un total de 8 épocas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWtaALBbwab1"
      },
      "source": [
        "# Compile the teacher model\n",
        "teacher.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Notice from_logits param is set to True\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "\n",
        "# Fit the model and save the training history (will take from 5 to 10 minutes depending on the GPU you were assigned to)\n",
        "teacher_history = teacher.fit(train_batches, epochs=8, validation_data=validation_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kSMig49wab2"
      },
      "source": [
        "## Entrene a un alumno desde cero como referencia\n",
        "\n",
        "Para evaluar la eficacia del proceso de destilación, entrene un modelo equivalente al alumno pero sin realizar la destilación de conocimientos. Observe que el entrenamiento se realiza durante sólo 5 épocas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPb3wE2nwab3"
      },
      "source": [
        "# Create student_scratch model with the same characteristics as the original student\n",
        "student_scratch = create_small_model()\n",
        "\n",
        "# Compile it\n",
        "student_scratch.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "\n",
        "# Train and evaluate student trained from scratch (will take around 3 mins with GPU enabled)\n",
        "student_scratch_history = student_scratch.fit(train_batches, epochs=5, validation_data=validation_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BdD9K57wab2"
      },
      "source": [
        "## Destilación de Conocimientos\n",
        "\n",
        "Para realizar el proceso de destilación de conocimiento usted utilizará el modelo personalizado que codificó anteriormente. Para ello, comience creando una instancia de la clase `Distiller` y pasándole los modelos del alumno y del profesor. A continuación, compílelo con los parámetros adecuados y entrénelo.\n",
        "\n",
        "Los dos modelos de estudiante se entrenan sólo durante 5 épocas, a diferencia del modelo de profesor, que se entrena durante 8 épocas. Esto se hace para mostrar que la destilación del conocimiento permite tiempos de entrenamiento más rápidos ya que el estudiante aprende de un modelo ya entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7EqhGlAwab2"
      },
      "source": [
        "# Create Distiller instance\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "\n",
        "# Compile Distiller model\n",
        "distiller.compile(\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.05,\n",
        "    temperature=5,\n",
        ")\n",
        "\n",
        "# Distill knowledge from teacher to student (will take around 3 mins with GPU enabled)\n",
        "distiller_history = distiller.fit(train_batches, epochs=5, validation_data=validation_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voTxT0cIxCYx"
      },
      "source": [
        "## Comparación de los modelos\n",
        "\n",
        "Para comparar los modelos, puede comprobar la \"precisión categórica dispersa\" de cada uno de ellos en el conjunto de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O4xXZlhxp92"
      },
      "source": [
        "# Compute accuracies\n",
        "student_scratch_acc = student_scratch.evaluate(test_batches, return_dict=True).get(\"sparse_categorical_accuracy\")\n",
        "distiller_acc = distiller.evaluate(test_batches, return_dict=True).get(\"sparse_categorical_accuracy\")\n",
        "teacher_acc = teacher.evaluate(test_batches, return_dict=True).get(\"sparse_categorical_accuracy\")\n",
        "\n",
        "# Print results\n",
        "print(f\"\\n\\nTeacher achieved a sparse_categorical_accuracy of {teacher_acc*100:.2f}%.\\n\")\n",
        "print(f\"Student with knowledge distillation achieved a sparse_categorical_accuracy of {distiller_acc*100:.2f}%.\\n\")\n",
        "print(f\"Student without knowledge distillation achieved a sparse_categorical_accuracy of {student_scratch_acc*100:.2f}%.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTDRmrXWwab3"
      },
      "source": [
        "El modelo del profesor ofrece una mayor precisión que los dos modelos de los alumnos. Esto es de esperar, ya que se entrenó durante más épocas y se utilizó una arquitectura más grande.\n",
        "\n",
        "Observa que el estudiante sin destilación fue superado por el estudiante con destilación de conocimiento. \n",
        "\n",
        "Como guardaste el historial de entrenamiento de cada modelo, puedes crear un gráfico para comparar mejor los dos modelos de los estudiantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-m8dvwS92rF"
      },
      "source": [
        "# Get relevant metrics from a history\n",
        "def get_metrics(history):\n",
        "  history = history.history\n",
        "  acc = history['sparse_categorical_accuracy']\n",
        "  val_acc = history['val_sparse_categorical_accuracy']\n",
        "  return acc, val_acc\n",
        "\n",
        "\n",
        "# Plot training and evaluation metrics given a dict of histories\n",
        "def plot_train_eval(history_dict):\n",
        "  \n",
        "  metric_dict = {}\n",
        "\n",
        "  for k, v in history_dict.items():\n",
        "    acc, val_acc= get_metrics(v)\n",
        "    metric_dict[f'{k} training acc'] = acc\n",
        "    metric_dict[f'{k} eval acc'] = val_acc\n",
        "\n",
        "  acc_plot = pd.DataFrame(metric_dict)\n",
        "  \n",
        "  acc_plot = sns.lineplot(data=acc_plot, markers=True)\n",
        "  acc_plot.set_title('training vs evaluation accuracy')\n",
        "  acc_plot.set_xlabel('epoch')\n",
        "  acc_plot.set_ylabel('sparse_categorical_accuracy')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Plot for comparing the two student models\n",
        "plot_train_eval({\n",
        "    \"distilled\": distiller_history,\n",
        "    \"student_scratch\": student_scratch_history,\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm1VrbjK16n6"
      },
      "source": [
        "Este gráfico es muy interesante porque muestra que la versión destilada superó a la no modificada en casi todas las épocas al utilizar el conjunto de evaluación. Además, el alumno sin destilar obtiene una mayor precisión de entrenamiento, lo que indica que se está sobreajustando más que el modelo destilado. **Esto indica que el modelo destilado fue capaz de aprender de la regularización implementada por el profesor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGDr0PoC1nuP"
      },
      "source": [
        "-----------------------------\n",
        "**Ahora deberías tener una comprensión** más clara de lo que es la Destilación del Conocimiento y cómo se puede implementar utilizando Tensorflow y Keras. \n",
        "\n",
        "Este proceso es ampliamente utilizado para la compresión de modelos y ha demostrado funcionar muy bien. De hecho, puede que hayas oído hablar de [`DistilBert`](https://huggingface.co/transformers/model_doc/distilbert.html), que es una versión más pequeña, rápida, barata y ligera de BERT.\n",
        "\n",
        "\n",
        "**Sigue así.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKV8ha-3YJbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}